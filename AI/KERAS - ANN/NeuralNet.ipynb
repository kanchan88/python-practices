{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, dataFile, header=True, h=4):\n",
    "\n",
    "        # Take in the file\n",
    "        raw_input = pd.read_csv(dataFile)\n",
    "        # TODO checked: Remember to implement the preprocess method\n",
    "        processed_data = self.preprocessData(raw_input)\n",
    "\n",
    "        # Splitting the data to train and test\n",
    "        self.train_dataset, self.test_dataset = train_test_split(\n",
    "            processed_data)\n",
    "\n",
    "        # Getting number of columns and rows in train data set\n",
    "        ncols = len(self.train_dataset.columns)\n",
    "        nrows = len(self.train_dataset.index)\n",
    "\n",
    "        # Getting the Xs and Ys\n",
    "        self.X = self.train_dataset.iloc[:, [13,29,30,31]].values.reshape(nrows, 4)\n",
    "        self.y = self.train_dataset.iloc[:,[32]].values.reshape(nrows, 1)\n",
    "\n",
    "        # Find number of input and output layers from the dataset\n",
    "        input_layer_size = len(self.X[1])\n",
    "        if not isinstance(self.y[0], np.ndarray):\n",
    "            self.output_layer_size = 1\n",
    "        else:\n",
    "            self.output_layer_size = len(self.y[0])\n",
    "\n",
    "        # assign random weights to matrices in network\n",
    "        # number of weights connecting layers = (no. of nodes in previous layer) x (no. of nodes in following layer)\n",
    "        self.W_hidden = 2 * np.random.random((input_layer_size, h)) - 1\n",
    "        self.Wb_hidden = 2 * np.random.random((1, h)) - 1\n",
    "\n",
    "        self.W_output = 2 * np.random.random((h, self.output_layer_size)) - 1\n",
    "        self.Wb_output = np.ones((1, self.output_layer_size))\n",
    "\n",
    "        self.deltaOut = np.zeros((self.output_layer_size, 1))\n",
    "        self.deltaHidden = np.zeros((h, 1))\n",
    "        self.h = h\n",
    "\n",
    "    # Preprocessing raw data\n",
    "    def preprocessData(self, data):\n",
    "        data = data.dropna()\n",
    "        data = data.drop_duplicates()\n",
    "\n",
    "        # Creating correlation matrix\n",
    "        cor_matrix = data.corr().abs()\n",
    "\n",
    "        # Selecting upper triangle of correlation matrix\n",
    "        upper_tri = cor_matrix.where(\n",
    "            np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Finding index of feature columns with correlation greater than 0.95\n",
    "        to_drop = [column for column in upper_tri.columns if any(\n",
    "            upper_tri[column] > 0.95)]\n",
    "\n",
    "        # Dropping Marked Features\n",
    "        data = data.drop(data.columns[to_drop], axis=1)\n",
    "        return data\n",
    "\n",
    "    # TODOchecked marked\n",
    "    def __activation(self, x, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            self.__sigmoid(self, x)\n",
    "        if activation == \"ReLu\":\n",
    "            self.__relu(self, x)\n",
    "        if activation == \"tanh\":\n",
    "            self.__tanh(self, x)\n",
    "\n",
    "    # activation methods\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    # TODO checked: Define the derivative function for tanh, ReLu and their derivatives\n",
    "    def __activation_derivative(self, x, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            self.__sigmoid_derivative(self, x)\n",
    "        elif activation == \"ReLu\":\n",
    "            self.__relu_derivative(self, x)\n",
    "        elif activation == \"tanh\":\n",
    "            self.__tanh_derivative(self, x)\n",
    "\n",
    "    # derivative of sigmoid function, indicates confidence about existing weight\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def __relu_derivative(self, x):\n",
    "        return np.greater(0, x)\n",
    "\n",
    "    def __tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    # TRAINNING METHODS\n",
    "    def train(self, max_iterations=60000, learning_rate=0.25):\n",
    "        for iteration in range(max_iterations):\n",
    "            out = self.forward_pass(activation=\"sigmoid\")\n",
    "            out = self.forward_pass(activation=\"ReLu\")\n",
    "            out = self.forward_pass(activation=\"tanh\")\n",
    "\n",
    "            error = 0.5 * np.power((out - self.y), 2)\n",
    "            # TODO: I have coded the sigmoid activation, you have to do the rest\n",
    "            self.backward_pass(out, activation=\"sigmoid\")\n",
    "            self.backward_pass(out, activation=\"ReLu\")\n",
    "            self.backward_pass(out, activation=\"tanh\")\n",
    "\n",
    "            update_weight_output = learning_rate * \\\n",
    "                                   np.dot(self.X_hidden.T, self.deltaOut)\n",
    "            update_weight_output_b = learning_rate * \\\n",
    "                                     np.dot(np.ones((np.size(self.X, 0), 1)).T, self.deltaOut)\n",
    "\n",
    "            update_weight_hidden = learning_rate * \\\n",
    "                                   np.dot(self.X.T, self.deltaHidden)\n",
    "            update_weight_hidden_b = learning_rate * \\\n",
    "                                     np.dot(np.ones((np.size(self.X, 0), 1)).T, self.deltaHidden)\n",
    "\n",
    "            self.W_output += update_weight_output\n",
    "            self.Wb_output += update_weight_output_b\n",
    "            self.W_hidden += update_weight_hidden\n",
    "            self.Wb_hidden += update_weight_hidden_b\n",
    "\n",
    "        print(\"After \" + str(max_iterations) +\n",
    "              \" iterations, the total error is \" + str(np.sum(error)))\n",
    "        print(\"The final weight vectors are (starting from input to output layers) \\n\" + str(self.W_hidden))\n",
    "        print(\"The final weight vectors are (starting from input to output layers) \\n\" + str(self.W_output))\n",
    "\n",
    "        print(\"The final bias vectors are (starting from input to output layers) \\n\" +\n",
    "              str(self.Wb_hidden))\n",
    "        print(\"The final bias vectors are (starting from input to output layers) \\n\" +\n",
    "              str(self.Wb_output))\n",
    "\n",
    "    def forward_pass(self, activation):\n",
    "        # pass our inputs through our neural network\n",
    "        in_hidden = np.dot(self.X, self.W_hidden) + self.Wb_hidden\n",
    "        # TODO: I have coded the sigmoid activation, you have to do the rest\n",
    "        if activation == \"sigmoid\":\n",
    "            self.X_hidden = self.__sigmoid(in_hidden)\n",
    "        in_output = np.dot(self.X_hidden, self.W_output) + self.Wb_output\n",
    "        if activation == \"sigmoid\":\n",
    "            out = self.__sigmoid(in_output)\n",
    "\n",
    "        if activation == \"ReLu\":\n",
    "            self.X_hidden = self.__relu(in_hidden)\n",
    "        in_output = np.dot(self.X_hidden, self.W_output) + self.Wb_output\n",
    "        if activation == \"ReLu\":\n",
    "            out = self.__relu(in_output)\n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.X_hidden = self.__tanh(in_hidden)\n",
    "        in_output = np.dot(self.X_hidden, self.W_output) + self.Wb_output\n",
    "        if activation == \"tanh\":\n",
    "            out = self.__tanh(in_output)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward_pass(self, out, activation):\n",
    "        # pass our inputs through our neural network\n",
    "        self.compute_output_delta(out, activation)\n",
    "        self.compute_hidden_delta(activation)\n",
    "\n",
    "    # TODO: Implement other activation functions\n",
    "\n",
    "    def compute_output_delta(self, out, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            delta_output = (self.y - out) * (self.__sigmoid_derivative(out))\n",
    "        elif activation == \"ReLu\":\n",
    "            delta_output = (self.y - out) * (self.__relu_derivative(out))\n",
    "        elif activation == \"tanh\":\n",
    "            delta_output = (self.y - out) * (self.__tanh_derivative(out))\n",
    "\n",
    "        self.deltaOut = delta_output\n",
    "\n",
    "    def compute_hidden_delta(self, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            delta_hidden_layer = (self.deltaOut.dot(\n",
    "                self.W_output.T)) * (self.__sigmoid_derivative(self.X_hidden))\n",
    "        elif activation == \"ReLu\":\n",
    "            delta_hidden_layer = (self.deltaOut.dot(\n",
    "                self.W_output.T)) * (self.__relu_derivative(self.X_hidden))\n",
    "        elif activation == \"tanh\":\n",
    "            delta_hidden_layer = (self.deltaOut.dot(\n",
    "                self.W_output.T)) * (self.__tanh_derivative(self.X_hidden))\n",
    "\n",
    "        self.deltaHidden = delta_hidden_layer\n",
    "\n",
    "    # TODO: Implement the predict function for applying the trained model on the  test dataset.\n",
    "    # You can assume that the test dataset has the same format as the training dataset\n",
    "    # You have to output the test error from this function\n",
    "\n",
    "    def predict(self, header=True):        \n",
    "        test_cols = len(self.test_dataset.columns)\n",
    "        test_rows = len(self.test_dataset.index)\n",
    "        X = self.test_dataset.iloc[:, [13,29,30,31]].values.reshape(test_rows, 4)\n",
    "        y = self.test_dataset.iloc[:,[32]].values.reshape(test_rows, 1)\n",
    "        X = np.array(X)\n",
    "        W_hidden1 = np.array(self.W_hidden)\n",
    "        y_hat = np.dot(X, W_hidden1) + self.Wb_hidden\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evazs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 60000 iterations, the total error is 16060.0\n",
      "The final weight vectors are (starting from input to output layers) \n",
      "[[ 1.37065173e+14  1.37070073e+14 -1.37068532e+14 -1.37081171e+14]\n",
      " [ 3.92889187e+14  3.92903232e+14 -3.92898816e+14 -3.92935044e+14]\n",
      " [ 7.79087734e+14  7.79115586e+14 -7.79106829e+14 -7.79178669e+14]\n",
      " [ 7.91493294e+14  7.91521589e+14 -7.91512693e+14 -7.91585676e+14]]\n",
      "The final weight vectors are (starting from input to output layers) \n",
      "[[ 17562695.66737175]\n",
      " [ 17563009.57934659]\n",
      " [-17562910.88075263]\n",
      " [-17563720.58987682]]\n",
      "The final bias vectors are (starting from input to output layers) \n",
      "[[ 6.47691020e+13  6.47714174e+13 -6.47706894e+13 -6.47766617e+13]]\n",
      "The final bias vectors are (starting from input to output layers) \n",
      "[[17563720.96786926]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (99,4) (4,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d889ab14ff89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtestError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Values = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-df2378f955ff>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, header)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mW_hidden1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mW_hidden1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWb_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (99,4) (4,4) "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Get data\n",
    "    trainData = \"https://raw.githubusercontent.com/daniel-le18/dataset/master/student-mat.csv\"\n",
    "\n",
    "    # Passing data to NeuralNet\n",
    "    neural_network = NeuralNet(trainData)\n",
    "\n",
    "    # Train data\n",
    "    neural_network.train()\n",
    "\n",
    "    # Predict\n",
    "    testError = neural_network.predict()\n",
    "    print(\"Test Values = \" + str(testError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
